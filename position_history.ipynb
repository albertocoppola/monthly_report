{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e0eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import pandas_market_calendars as mcal\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f8442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtro de datas\n",
    "start_date = pd.Timestamp(date(date.today().year, 5, 30))\n",
    "end_date = pd.Timestamp(date.today())\n",
    "\n",
    "# Calendário de feriados da b3\n",
    "B3 = mcal.get_calendar('B3')\n",
    "B3_holidays = B3.holidays()\n",
    "\n",
    "# Filtro correto por range\n",
    "feriados = [h for h in B3_holidays.holidays if start_date <= h <= end_date]\n",
    "\n",
    "# Define um CustomBusinessDay com esses feriados\n",
    "bday_brasil = pd.offsets.CustomBusinessDay(holidays=feriados)\n",
    "\n",
    "# data d-2\n",
    "d2_date = (date.today()-2*bday_brasil).strftime('%Y-%m-%d')\n",
    "# d2_date = '2025-06-02'\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=d2_date, freq='B')\n",
    "date_range_no_holidays = date_range[~date_range.isin(feriados)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c0f156",
   "metadata": {},
   "source": [
    "Configuração de API e funções que coletam os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc8fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User configs for API authentication\n",
    "api_acces_id = \"89297662e92386720e192e56ffdc0d5e.access\"\n",
    "api_secret = \"b8b3cfabf25982a64a1074360f83b0dc143aa5bd75560abf5c901b0977364de4\"\n",
    "api_password = \"juNTr1QbtbY9NZ8ACrMF\"\n",
    "user_name = \"alberto.coppola@perenneinvestimentos.com.br\"\n",
    "\n",
    "columns_filter = ['portfolio_id','overview_date', 'instrument_name', 'instrument_id','book_name', 'instrument_type', 'quantity', 'price', 'asset_value','exposure_value','dtd_ativo_fin']\n",
    "\n",
    "\n",
    "# Retorna um dataframe a partir da parametros\n",
    "def fetch_data(url: str, params: dict, access_id, secret, user_name, api_password):\n",
    "    # trocar para a URL do ambiente desejado\n",
    "    base_url = \"https://perenne.bluedeck.com.br/api\"\n",
    "    url_token = \"auth/token\"\n",
    "    \n",
    "    # trocar o e-mail e a senha de aplicação\n",
    "    data = {\"username\": user_name, \"password\": api_password}\n",
    "\n",
    "    # trocar o ID e o secret\n",
    "    client_headers = {\n",
    "        \"CF-Access-Client-Id\": access_id,\n",
    "        \"CF-Access-Client-Secret\": secret\n",
    "    }\n",
    "\n",
    "    # realiza request\n",
    "    response = requests.post(f\"{base_url}/{url_token}\", data=data, headers=client_headers)\n",
    "\n",
    "    # Converte em json\n",
    "    json_response = response.json()\n",
    "\n",
    "    # token de acesso\n",
    "    access_token = json_response['access_token']\n",
    "    token_type = json_response['token_type']\n",
    "    # expiration_dt = json_response['expires_at']\n",
    "\n",
    "    # Definindo headers para realização de chamadas\n",
    "    client_header = {\n",
    "        \"CF-Access-Client-Id\": access_id,\n",
    "        \"CF-Access-Client-Secret\": secret\n",
    "    }\n",
    "    token_header = {\"Authorization\": f\"{token_type} {access_token}\"}\n",
    "    headers = {**client_header, **token_header}\n",
    "\n",
    "    response_api = requests.post(f\"{base_url}/{url}\", headers=headers, json=params)\n",
    "    response_json = response_api.json()\n",
    "    \n",
    "    return response_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52b97b",
   "metadata": {},
   "source": [
    "Busco todas posições de ativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a801dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de posições\n",
    "url = 'portfolio_position/positions/get'\n",
    "\n",
    "# define parametros de busca\n",
    "params = {\n",
    "    \"start_date\": \"2025-06-30\",\n",
    "    \"end_date\": d2_date,\n",
    "    \"portfolio_group_ids\": [1],\n",
    "}\n",
    "\n",
    "df_positions = fetch_data(url, params, api_acces_id, api_secret, user_name, api_password)['objects']\n",
    "df_positions = pd.DataFrame(df_positions)\n",
    "# Salva o de-para de portfolio_id e nome\n",
    "funds_nav = df_positions.loc[['portfolio_id','date','net_asset_value']].T.drop_duplicates()\n",
    "funds_nav.rename(columns={\"net_asset_value\":\"portfolio_nav\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2fafa",
   "metadata": {},
   "source": [
    "Busco todos os fundos que são explodidos e geridos internamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b78a709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define URL de acesso\n",
    "url_funds = 'portfolio_registration/portfolio_group/get'\n",
    "\n",
    "# define parametros de busca\n",
    "params_funds = {\"get_composition\": True}\n",
    "\n",
    "# Identifica todos os fundos do sistema para explodi-los\n",
    "all_funds = fetch_data(url_funds, params_funds, api_acces_id, api_secret, user_name, api_password)['objects']\n",
    "all_funds = pd.DataFrame(all_funds)\n",
    "\n",
    "all_funds = list(all_funds['8'].loc['composition_names'].values())\n",
    "\n",
    "funds_name = df_positions.loc[['portfolio_id','name']].T.drop_duplicates()\n",
    "funds_name.to_csv(\"funds_name.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a7109c",
   "metadata": {},
   "source": [
    "Histórico dos benchmarks (CDI, IBOV e IHFA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f449985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define URL de acesso\n",
    "url = 'market_data/pricing/prices/get'\n",
    "\n",
    "# define parametros de busca\n",
    "params = {\n",
    "  \"start_date\": \"2024-12-31\",\n",
    "  \"end_date\": d2_date,\n",
    "  \"instrument_ids\": [\n",
    "    1540,\n",
    "    1932,\n",
    "    9\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Identifica todos os fundos do sistema para explodi-los\n",
    "bench_df = fetch_data(url, params, api_acces_id, api_secret, user_name, api_password)\n",
    "bench_df = pd.DataFrame(bench_df['prices'])[['date','instrument','variation']]\n",
    "bench_df.variation = bench_df.variation.astype(np.float64)\n",
    "bench_df['date'] = pd.to_datetime(bench_df['date'])\n",
    "bench_df = bench_df.sort_values(['date'])\n",
    "\n",
    "# YTD percentual\n",
    "bench_df['ytd_pct'] = bench_df.groupby(\n",
    "    ['instrument'])['variation'].transform(lambda x: (1 + x).cumprod() - 1)\n",
    "bench_df.to_csv(\"benchmarks.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5a6c0",
   "metadata": {},
   "source": [
    "Busco por erros de PNL pelo sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d94908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro encontrado:\n",
      "             name        date\n",
      "0  CSHG YANKEE II  2025-07-08\n"
     ]
    }
   ],
   "source": [
    "url = 'portfolio_position/attribution/errors_view/get'\n",
    "\n",
    "for f,g in funds_name.iterrows():\n",
    "    if g['name'] in all_funds:\n",
    "\n",
    "        params = {\n",
    "        \"base_date\": d2_date,\n",
    "        \"consolidation_type\": 3,\n",
    "        \"show_errors\": True,\n",
    "        \"periods\": [\n",
    "            2,\n",
    "            3\n",
    "        ],\n",
    "        \"attribution_types\": [\n",
    "            1\n",
    "        ],\n",
    "        \"portfolio_ids\": [\n",
    "            g.portfolio_id\n",
    "        ]\n",
    "        }\n",
    "        errors = fetch_data(url, params, api_acces_id, api_secret, user_name, api_password)['objects']\n",
    "        errors = pd.DataFrame(errors).T\n",
    "\n",
    "        if len(errors)>=1:\n",
    "            errors = (errors).merge(funds_name,on='portfolio_id',how='left')[['name','date']]\n",
    "            print(\"Erro encontrado:\")\n",
    "            print(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60949bc",
   "metadata": {},
   "source": [
    "Construção do CSV de posições e custos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eaad398",
   "metadata": {},
   "outputs": [],
   "source": [
    "registros = pd.DataFrame()\n",
    "costs_df = pd.DataFrame()\n",
    "\n",
    "# Iterar pelas colunas de posição (ids)\n",
    "for data_col in df_positions.columns:\n",
    "\n",
    "    try:\n",
    "        # Pega os dados do id (coluna), vira Series\n",
    "        id_col = df_positions[data_col]\n",
    "\n",
    "        # Pega o portfolio_id\n",
    "        portfolio_id = id_col['portfolio_id']\n",
    "\n",
    "        # Pula o portfolio consolidado para evitar duplicidade \n",
    "        if portfolio_id == 49:\n",
    "            continue\n",
    "\n",
    "        # Pega o dicionário de instrument_positions e transforma em df\n",
    "        instrument_positions = pd.DataFrame(id_col['instrument_positions'])\n",
    "\n",
    "        # Pega o dicionário de custos e transforma em df\n",
    "        costs_position = pd.DataFrame(id_col['financial_transaction_positions'])\n",
    "\n",
    "        # Concatena o portfolio id\n",
    "        instrument_positions['portfolio_id'] = portfolio_id\n",
    "\n",
    "        # Busca a lista de pnl dentro da coluna Attribution e concatena pro df de instrument_position\n",
    "        pnl_ = []\n",
    "        for _, row in instrument_positions.iterrows():\n",
    "            try:\n",
    "                pnl_.append(np.float64(row.attribution['total']['financial_value']))\n",
    "            except:\n",
    "                pnl_.append(0.0)\n",
    "\n",
    "        instrument_positions['dtd_ativo_fin'] = pnl_\n",
    "\n",
    "        # Faz o mesmo pra custos\n",
    "        pnl_ = []\n",
    "        for _, row in costs_position.iterrows():\n",
    "            try:\n",
    "                pnl_.append(np.float64(row.attribution['total']['financial_value']))\n",
    "            except:\n",
    "                pnl_.append(0.0)\n",
    "\n",
    "        costs_position['dtd_custos_fin'] = pnl_\n",
    "\n",
    "        # Concatena a data nos custos\n",
    "        costs_position['overview_date'] = instrument_positions.overview_date.unique()[0]\n",
    "        costs_position['origin_portfolio_id'] = portfolio_id\n",
    "\n",
    "        # Concatena no df de registros\n",
    "        registros = pd.concat([registros,instrument_positions])\n",
    "\n",
    "        # Concatena no df de custos\n",
    "        costs_df = pd.concat([costs_df,costs_position])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar data {data_col}: {e}\")\n",
    "    \n",
    "\n",
    "# Filtro pelas colunas de interesse e transforma quantidade, preço e aum em float\n",
    "registros = registros[columns_filter]\n",
    "registros.loc[:,['quantity','asset_value','exposure_value','price','dtd_ativo_fin']] = registros[['quantity','asset_value','exposure_value','price','dtd_ativo_fin']].astype(np.float64)\n",
    "\n",
    "# Transforma a coluna de custos em float e o id em int\n",
    "costs_df['financial_value']=costs_df['financial_value'].astype(float)\n",
    "\n",
    "# Filtro as colunas de custos\n",
    "costs_df = costs_df[['financial_value','attribution','book_name','category_name','origin_portfolio_id','origin_accounting_transaction_id','dtd_custos_fin','overview_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e24e6d",
   "metadata": {},
   "source": [
    "Uso interno para pegar books e registros duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a20d545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exceções\n",
    "registros['book_name'] = registros['book_name'].replace(\"Risco >> HYPE >> Ação HYPE\", \"Risco >> HYPE\")\n",
    "registros['book_name'] = registros['book_name'].replace(\"Risco >> HYPE >> Opção HYPE\", \"Risco >> HYPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dad9599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "instrument_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "book_name",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "217e7279-f145-458d-a62f-d1f6210c5d56",
       "rows": [],
       "shape": {
        "columns": 3,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>instrument_name</th>\n",
       "      <th>book_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, instrument_name, book_name]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#informaçoes de cadastro de book dos fundos que explodimos\n",
    "dup = registros.merge(funds_name,on='portfolio_id',how='left')[['name','instrument_name','book_name']].drop_duplicates()\n",
    "dup = dup[dup['name'].isin(all_funds)]\n",
    "\n",
    "# Conta quantos books diferentes cada (portfolio_id, instrument_name) tem\n",
    "duplicados = (\n",
    "    dup\n",
    "    .groupby(['name', 'instrument_name'])['book_name']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Seleciona só os que aparecem em 2 ou mais books\n",
    "duplicados = duplicados[duplicados['book_name'] > 1][['name', 'instrument_name']]\n",
    "duplicados = duplicados.merge(dup,on=['name','instrument_name'],how='left')\n",
    "display(duplicados)\n",
    "\n",
    "\"\"\"\n",
    "----------- Filtro de contingencia enquanto não arruma as categorias no sistema ---------\n",
    "\n",
    "        soma as quantidades e notional pegando o primeiro book name para categorização\n",
    "\"\"\"\n",
    "registros = registros.groupby(['overview_date','portfolio_id','instrument_name']).agg({\n",
    "    'instrument_id':'first',\n",
    "    'book_name':'first',\n",
    "    'instrument_type':'first',\n",
    "    'quantity':'sum',\n",
    "    'price':'first',\n",
    "    'asset_value':'sum',\n",
    "    'exposure_value':'sum',\n",
    "    'dtd_ativo_fin': 'sum'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55e218",
   "metadata": {},
   "source": [
    "Flatten o csv de posições em um novo csv 'explodido'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f7bd5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciono uma coluna com o NAV do fundo para o calculo de PNL\n",
    "registros = pd.merge(registros,funds_nav,left_on=['portfolio_id','overview_date'],right_on=['portfolio_id','date']).drop(columns='date')\n",
    "registros.rename(columns={\"net_asset_value\":\"portfolio_nav\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31e894f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AlbertoCoppola\\AppData\\Local\\Temp\\ipykernel_17908\\3120038746.py:37: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mult = notional/aum\n",
      "C:\\Users\\AlbertoCoppola\\AppData\\Local\\Temp\\ipykernel_17908\\3120038746.py:37: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mult = notional/aum\n",
      "C:\\Users\\AlbertoCoppola\\AppData\\Local\\Temp\\ipykernel_17908\\3120038746.py:37: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mult = notional/aum\n"
     ]
    }
   ],
   "source": [
    "def explodir_portfolio(portfolio_id, data, todas_posicoes, todos_custos, visitados=None, notional = None, portfolio_origem_id=None, nivel = 0):\n",
    "    \"\"\"\n",
    "    - portfolio_id: o portfólio que estamos processando\n",
    "    - data: a data da posição\n",
    "    - todas_posicoes: DataFrame com todas as posições\n",
    "    - multiplicador: proporção da posição herdada\n",
    "    \"\"\"\n",
    "    \n",
    "    if visitados is None:\n",
    "        visitados = set()\n",
    "        # print(\"STARTING: \",portfolio_id)\n",
    "    if portfolio_id in visitados:\n",
    "        return [], pd.DataFrame()\n",
    "\n",
    "    visitados.add(portfolio_id)\n",
    "\n",
    "    # Filtra as posições desse portfolio na data\n",
    "    posicoes = todas_posicoes[\n",
    "        (todas_posicoes['overview_date'] == data) &\n",
    "        (todas_posicoes['portfolio_id'] == portfolio_id)\n",
    "    ]\n",
    "\n",
    "    # Filtra os custos do portfolio na data\n",
    "    custos = todos_custos[\n",
    "        (todos_custos['overview_date'] == data) &\n",
    "        (todos_custos['origin_portfolio_id'] == portfolio_id)\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Calculo do AUM total do fundo, soma ativos + custos\n",
    "    aum = posicoes.asset_value.sum() + custos.financial_value.sum()\n",
    "    # print(aum)\n",
    "    if notional is None:\n",
    "        notional = aum\n",
    "    # print(notional)\n",
    "    # Calculo um multiplicador para proporcionalizar as posições\n",
    "    mult = notional/aum\n",
    "\n",
    "    if portfolio_origem_id is None:\n",
    "        portfolio_origem_id = portfolio_id\n",
    "        # exposure = 1\n",
    "    else:\n",
    "        # print(\"Multiplicando por: \", round(exposure*100,4), \"%\")\n",
    "        posicoes.loc[:,['quantity','asset_value','exposure_value','dtd_ativo_fin']] = posicoes[['quantity','asset_value','exposure_value','dtd_ativo_fin']] * mult\n",
    "        custos.loc[:,['financial_value','dtd_custos_fin']] = custos.loc[:,['financial_value','dtd_custos_fin']] * mult\n",
    "\n",
    "    # Seta o portfolio_id para a origem e cria na tabela de custos\n",
    "    posicoes.loc[:,['portfolio_id']] = portfolio_origem_id\n",
    "    custos.loc[:,['root_portfolio']] = portfolio_origem_id\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for _, row in posicoes.iterrows():\n",
    "        row_portfolio_id = row['instrument_id']\n",
    "        if row.instrument_name in (all_funds):  # Checa se a linha é um fundo\n",
    "            # print(row.instrument_name)\n",
    "            # É um fundo investido, explodir recursivamente\n",
    "            sub_resultados, resultado_custo = explodir_portfolio(\n",
    "                row_portfolio_id,\n",
    "                data,\n",
    "                todas_posicoes,\n",
    "                todos_custos,\n",
    "                visitados=visitados,\n",
    "                notional = np.float64(row.asset_value),\n",
    "                portfolio_origem_id=portfolio_origem_id,\n",
    "                nivel = nivel + 1\n",
    "            )\n",
    "            resultados += sub_resultados\n",
    "            # Concatena no df de custos\n",
    "            custos = pd.concat([custos,resultado_custo])\n",
    "        else:\n",
    "            novo = row.copy()\n",
    "            novo['portfolio_origem'] = portfolio_id\n",
    "            novo[\"nivel\"] = nivel\n",
    "            resultados.append(novo)\n",
    "    \n",
    "    return resultados, custos\n",
    "\n",
    "todas_explodidas = pd.DataFrame()\n",
    "todos_custos_explodidos = pd.DataFrame()\n",
    "\n",
    "datas = registros.overview_date.unique()\n",
    "portfolios = registros['portfolio_id'].unique()\n",
    "\n",
    "for data in datas:\n",
    "    for portfolio in portfolios:\n",
    "        explodido, custo = explodir_portfolio(portfolio, data, registros, costs_df)\n",
    "        todas_explodidas = pd.concat([todas_explodidas,pd.DataFrame(explodido)])\n",
    "        todos_custos_explodidos = pd.concat([todos_custos_explodidos,custo])\n",
    "df_explodido = pd.DataFrame(todas_explodidas)\n",
    "\n",
    "# Subscrevo o portfolio_nav para referenciar ao portfolio_id original\n",
    "df_explodido = pd.merge(df_explodido.drop(columns=['portfolio_nav']),funds_nav,left_on=['portfolio_id','overview_date'],right_on=['portfolio_id','date'],how='left').drop(columns=['date'])\n",
    "\n",
    "todos_custos_explodidos = todos_custos_explodidos.groupby(['overview_date','root_portfolio','origin_portfolio_id','category_name']).agg({\n",
    "    'book_name':'first',\n",
    "    'financial_value':'sum',\n",
    "    'dtd_custos_fin':'sum'\n",
    "}).reset_index()\n",
    "todos_custos_explodidos['overview_date'] = pd.to_datetime(todos_custos_explodidos['overview_date'])\n",
    "\n",
    "# df_explodido['pct_exposicao'] = df_explodido['exposure_value'].astype(float) / df_explodido['portfolio_nav'].astype(float)\n",
    "\n",
    "# df_explodido.drop(columns='instrument_name',inplace=True)\n",
    "df_explodido['overview_date'] = pd.to_datetime(df_explodido['overview_date'])\n",
    "\n",
    "df_explodido = df_explodido.groupby(['overview_date','portfolio_id','instrument_name','instrument_id']).agg({\n",
    "        'book_name':'first',\n",
    "        'asset_value':'sum',\n",
    "        'dtd_ativo_fin':'sum',\n",
    "        'exposure_value':'sum'\n",
    "    }).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1168a6",
   "metadata": {},
   "source": [
    "Tratamento de exceção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00c8ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posiçoes fora do relatorio e custos viram mesmo book e sao concatenados a partir de 30 de junho\n",
    "todos_custos_explodidos.loc[:,'book_name'] = 'Risco >> Caixas e Provisionamentos >> CPR (Provisões)'\n",
    "todos_custos_explodidos.rename(columns={'root_portfolio':'portfolio_id','category_name':'instrument_name','dtd_custos_fin':'dtd_ativo_fin','financial_value':'asset_value'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b806b14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading feed file...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Reading feed file...\")\n",
    "    britechdf = pd.read_csv('feed_britech.csv',encoding='latin',parse_dates=True).dropna(how='all')\n",
    "    britechdf[['asset_value','dtd_ativo_pct','dtd_ativo_fin','exposure_value']]=britechdf[['asset_value','dtd_ativo_pct','dtd_ativo_fin','exposure_value']].astype(float)\n",
    "    britechdf['overview_date'] = pd.to_datetime(britechdf['overview_date'])\n",
    "\n",
    "    betacurve = pd.read_csv('beta_curva.csv',encoding='latin',parse_dates=True).dropna(how='all')\n",
    "    betacurve[['asset_value','dtd_ativo_pct','dtd_ativo_fin','exposure_value']]=betacurve[['asset_value','dtd_ativo_pct','dtd_ativo_fin','exposure_value']].astype(float)\n",
    "    betacurve['overview_date'] = pd.to_datetime(betacurve['overview_date'])\n",
    "except Exception as e:\n",
    "        input(f\"{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "759e1149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AlbertoCoppola\\AppData\\Local\\Temp\\ipykernel_17908\\3977027211.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_positions['exposure_value_ontem'] = (\n",
      "C:\\Users\\AlbertoCoppola\\AppData\\Local\\Temp\\ipykernel_17908\\3977027211.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_positions['asset_value_ontem'] = (\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.DataFrame()\n",
    "for ptf in britechdf.portfolio_id.unique():\n",
    "    \"\"\"\n",
    "    - filtra o dataframe de posições de acordo com o csv de feed (britechdf).\n",
    "    - constrói um novo dataframe reorganizando e concatenando os books de posições e custos\n",
    "    - esse dataframe é o de caixa e provisionamentos da carteira\n",
    "    \"\"\"\n",
    "    filtered_britechdf = britechdf[britechdf.portfolio_id==ptf]\n",
    "    filtered_beta = betacurve[betacurve.portfolio_id==ptf]\n",
    "\n",
    "    df_cpr = df_explodido[~(df_explodido['book_name'].isin(list(filtered_britechdf.book_name.drop_duplicates()))) & \n",
    "    (df_explodido['portfolio_id']== ptf) &\n",
    "    ~(df_explodido['book_name'].str.lower().str.startswith('off'))]\n",
    "\n",
    "    df_cpr.loc[:,'book_name'] = 'Risco >> Caixas e Provisionamentos >> CPR (Provisões)'\n",
    "\n",
    "    df_cpr = pd.concat([df_cpr,todos_custos_explodidos[todos_custos_explodidos.portfolio_id==ptf]],ignore_index=True)\n",
    "    df_cpr = df_cpr.loc[df_cpr['overview_date'] > pd.Timestamp('2025-06-30')]\n",
    "    \n",
    "    df_cpr['asset_value_ontem'] = (\n",
    "    df_cpr.groupby(['portfolio_id','instrument_name'])['asset_value']\n",
    "    .shift(1)\n",
    ")\n",
    "\n",
    "    df_cpr = df_cpr.groupby(['overview_date','portfolio_id','instrument_name']).agg({\n",
    "        'book_name':'first',\n",
    "        'asset_value':'sum',\n",
    "        'asset_value_ontem':'sum',\n",
    "        'dtd_ativo_fin':'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    cpr_britech = filtered_britechdf[(filtered_britechdf['book_name']=='Risco >> Caixas e Provisionamentos')]\n",
    "    df_cpr = pd.concat([df_cpr,cpr_britech],ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    df_positions = df_explodido[(df_explodido['book_name'].isin(list(filtered_britechdf.book_name.drop_duplicates()))) & \n",
    "                    (df_explodido['portfolio_id']== ptf)]\n",
    "     \n",
    "    df_positions['exposure_value_ontem'] = (\n",
    "        df_positions.groupby(['portfolio_id','instrument_name'])['exposure_value']\n",
    "        .shift(1)\n",
    "    )\n",
    "\n",
    "    df_positions['asset_value_ontem'] = (\n",
    "        df_positions.groupby(['portfolio_id','instrument_name'])['asset_value']\n",
    "        .shift(1)\n",
    "    )\n",
    "    df_positions = df_positions.dropna()\n",
    "\n",
    "    pos_britech = filtered_britechdf[~(filtered_britechdf['book_name']=='Risco >> Caixas e Provisionamentos')]\n",
    "    df_positions = pd.concat([df_positions,pos_britech],ignore_index=True).reset_index(drop=True)\n",
    "    df_positions = pd.concat([df_positions,filtered_beta],ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    df_positions = df_positions.sort_values(['portfolio_id','instrument_name','overview_date'])\n",
    "\n",
    "    # Calculo do pnl diário do ativo e da carteira\n",
    "    df_positions['dtd_ativo_pct'] = df_positions['dtd_ativo_fin'] / df_positions['exposure_value_ontem']\n",
    "\n",
    "    df_positions['Year'] = df_positions['overview_date'].dt.year\n",
    "    df_positions['Month'] = df_positions['overview_date'].dt.month\n",
    "\n",
    "    # MTD percentual\n",
    "    df_positions['mtd_ativo_pct'] = df_positions.groupby(\n",
    "        ['portfolio_id','instrument_name','Year','Month'])['dtd_ativo_pct'].transform(lambda x: (1 + x).cumprod() - 1)\n",
    "\n",
    "    # YTD percentual\n",
    "    df_positions['ytd_ativo_pct'] = df_positions.groupby(\n",
    "        ['portfolio_id','instrument_name','Year'])['dtd_ativo_pct'].transform(lambda x: (1 + x).cumprod() - 1)\n",
    "\n",
    "    df_positions = pd.concat([df_positions,df_cpr],ignore_index=True)\n",
    "    df_positions['overview_date'] = pd.to_datetime(df_positions['overview_date'])\n",
    "    df_positions = df_positions.fillna(0)\n",
    "\n",
    "    df_final = pd.concat([df_final,df_positions])\n",
    "df_final = df_final.sort_values(['portfolio_id','instrument_name','overview_date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b28666",
   "metadata": {},
   "source": [
    "Quebra os grupos em colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be31c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split da coluna pelo separador \" >> \"\n",
    "df_split = df_final['book_name'].str.split(' >> ', expand=True)\n",
    "\n",
    "# Renomeia as colunas de acordo com a quantidade de splits encontrados\n",
    "df_split.columns = [f'grupo_{i+1}' for i in range(df_split.shape[1])]\n",
    "\n",
    "groups_df = pd.concat([df_split,df_final['book_name']],axis=1).drop_duplicates()\n",
    "groups_df = pd.concat([groups_df,pd.DataFrame({'grupo_1':['Caixa','Risco'],'book_name':['Caixa','Risco']})])\n",
    "\n",
    "df_all = pd.concat([df_final, df_split], axis=1)\n",
    "df_all = df_all.replace(np.inf,0).replace(-np.inf,0)\n",
    "\n",
    "groups_df.fillna(\"\").to_csv(\"groups.csv\", index=False)\n",
    "df_all.to_csv('positions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
