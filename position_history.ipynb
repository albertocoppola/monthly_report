{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a4e0eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import pandas_market_calendars as mcal\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "86f8442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtro de datas\n",
    "start_date = pd.Timestamp(date(date.today().year, 5, 30))\n",
    "end_date = pd.Timestamp(date.today())\n",
    "\n",
    "# Calendário de feriados da b3\n",
    "B3 = mcal.get_calendar('B3')\n",
    "B3_holidays = B3.holidays()\n",
    "\n",
    "# Filtro correto por range\n",
    "feriados = [h for h in B3_holidays.holidays if start_date <= h <= end_date]\n",
    "\n",
    "# Define um CustomBusinessDay com esses feriados\n",
    "bday_brasil = pd.offsets.CustomBusinessDay(holidays=feriados)\n",
    "\n",
    "# data d-2\n",
    "d2_date = (date.today()-2*bday_brasil).strftime('%Y-%m-%d')\n",
    "# d2_date = '2025-06-02'\n",
    "\n",
    "date_range = pd.date_range(start=start_date, end=d2_date, freq='B')\n",
    "date_range_no_holidays = date_range[~date_range.isin(feriados)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c0f156",
   "metadata": {},
   "source": [
    "Configuração de API e funções que coletam os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2dc8fa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User configs for API authentication\n",
    "api_acces_id = \"89297662e92386720e192e56ffdc0d5e.access\"\n",
    "api_secret = \"b8b3cfabf25982a64a1074360f83b0dc143aa5bd75560abf5c901b0977364de4\"\n",
    "api_password = \"juNTr1QbtbY9NZ8ACrMF\"\n",
    "user_name = \"alberto.coppola@perenneinvestimentos.com.br\"\n",
    "\n",
    "columns_filter = ['portfolio_id','overview_date', 'instrument_name', 'instrument_id','book_name', 'instrument_type', 'quantity', 'price', 'asset_value','exposure_value','dtd_ativo_fin']\n",
    "\n",
    "\n",
    "# Retorna um dataframe a partir da parametros\n",
    "def fetch_data(url: str, params: dict, access_id, secret, user_name, api_password):\n",
    "    # trocar para a URL do ambiente desejado\n",
    "    base_url = \"https://perenne.bluedeck.com.br/api\"\n",
    "    url_token = \"auth/token\"\n",
    "    \n",
    "    # trocar o e-mail e a senha de aplicação\n",
    "    data = {\"username\": user_name, \"password\": api_password}\n",
    "\n",
    "    # trocar o ID e o secret\n",
    "    client_headers = {\n",
    "        \"CF-Access-Client-Id\": access_id,\n",
    "        \"CF-Access-Client-Secret\": secret\n",
    "    }\n",
    "\n",
    "    # realiza request\n",
    "    response = requests.post(f\"{base_url}/{url_token}\", data=data, headers=client_headers)\n",
    "\n",
    "    # Converte em json\n",
    "    json_response = response.json()\n",
    "\n",
    "    # token de acesso\n",
    "    access_token = json_response['access_token']\n",
    "    token_type = json_response['token_type']\n",
    "    # expiration_dt = json_response['expires_at']\n",
    "\n",
    "    # Definindo headers para realização de chamadas\n",
    "    client_header = {\n",
    "        \"CF-Access-Client-Id\": access_id,\n",
    "        \"CF-Access-Client-Secret\": secret\n",
    "    }\n",
    "    token_header = {\"Authorization\": f\"{token_type} {access_token}\"}\n",
    "    headers = {**client_header, **token_header}\n",
    "\n",
    "    response_api = requests.post(f\"{base_url}/{url}\", headers=headers, json=params)\n",
    "    try:\n",
    "        response_json = response_api.json()['objects']\n",
    "    except:\n",
    "        response_json = response_api.json()['variations']\n",
    "    \n",
    "    return pd.DataFrame(response_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52b97b",
   "metadata": {},
   "source": [
    "Busco todas posições de ativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a801dacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de posições\n",
    "url = 'portfolio_position/positions/get'\n",
    "\n",
    "# define parametros de busca\n",
    "params = {\n",
    "    \"start_date\": \"2025-06-25\",\n",
    "    \"end_date\": d2_date,\n",
    "    # \"instrument_position_aggregation\": 3\n",
    "    \"portfolio_group_ids\": [1],\n",
    "    # \"portfolio_ids\": [1175,1517]  # ID do Fundo,\n",
    "    # \"book_depth\": 5,\n",
    "}\n",
    "\n",
    "df_positions = fetch_data(url, params, api_acces_id, api_secret, user_name, api_password)\n",
    "\n",
    "# Salva o de-para de portfolio_id e nome\n",
    "funds_nav = df_positions.loc[['portfolio_id','date','net_asset_value']].T.drop_duplicates()\n",
    "funds_nav.rename(columns={\"net_asset_value\":\"portfolio_nav\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2fafa",
   "metadata": {},
   "source": [
    "Busco todos os fundos que são explodidos e geridos internamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b78a709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define URL de acesso\n",
    "url_funds = 'portfolio_registration/portfolio_group/get'\n",
    "\n",
    "# define parametros de busca\n",
    "params_funds = {\"get_composition\": True}\n",
    "\n",
    "# Identifica todos os fundos do sistema para explodi-los\n",
    "all_funds = fetch_data(url_funds, params_funds, api_acces_id, api_secret, user_name, api_password)\n",
    "\n",
    "all_funds = list(all_funds['8'].loc['composition_names'].values())\n",
    "\n",
    "funds_name = df_positions.loc[['portfolio_id','name']].T.drop_duplicates()\n",
    "funds_name.to_csv(\"funds_name.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ec462211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Carteira 4']\n"
     ]
    }
   ],
   "source": [
    "missing_funds = [name for name in all_funds if name not in df_positions.loc['name'].unique()]\n",
    "print(missing_funds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60949bc",
   "metadata": {},
   "source": [
    "Construção do CSV de posições e custos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5eaad398",
   "metadata": {},
   "outputs": [],
   "source": [
    "registros = pd.DataFrame()\n",
    "costs_df = pd.DataFrame()\n",
    "\n",
    "# Iterar pelas colunas de posição (ids)\n",
    "for data_col in df_positions.columns:\n",
    "\n",
    "    try:\n",
    "        # Pega os dados do id (coluna), vira Series\n",
    "        id_col = df_positions[data_col]\n",
    "\n",
    "        # Pega o portfolio_id\n",
    "        portfolio_id = id_col['portfolio_id']\n",
    "\n",
    "        # Pula o portfolio consolidado para evitar duplicidade \n",
    "        if portfolio_id == 49:\n",
    "            continue\n",
    "\n",
    "        # Pega o dicionário de instrument_positions e transforma em df\n",
    "        instrument_positions = pd.DataFrame(id_col['instrument_positions'])\n",
    "\n",
    "        # Pega o dicionário de custos e transforma em df\n",
    "        costs_position = pd.DataFrame(id_col['financial_transaction_positions'])\n",
    "\n",
    "        # Concatena o portfolio id\n",
    "        instrument_positions['portfolio_id'] = portfolio_id\n",
    "\n",
    "        # Busca a lista de pnl dentro da coluna Attribution e concatena pro df de instrument_position\n",
    "        pnl_ = []\n",
    "        for _, row in instrument_positions.iterrows():\n",
    "            try:\n",
    "                pnl_.append(np.float64(row.attribution['total']['financial_value']))\n",
    "            except:\n",
    "                pnl_.append(0.0)\n",
    "\n",
    "        instrument_positions['dtd_ativo_fin'] = pnl_\n",
    "\n",
    "        # Faz o mesmo pra custos\n",
    "        pnl_ = []\n",
    "        for _, row in costs_position.iterrows():\n",
    "            try:\n",
    "                pnl_.append(np.float64(row.attribution['total']['financial_value']))\n",
    "            except:\n",
    "                pnl_.append(0.0)\n",
    "\n",
    "        costs_position['dtd_custos_fin'] = pnl_\n",
    "\n",
    "        # Concatena a data nos custos\n",
    "        costs_position['overview_date'] = instrument_positions.overview_date.unique()[0]\n",
    "        costs_position['origin_portfolio_id'] = portfolio_id\n",
    "\n",
    "        # Concatena no df de registros\n",
    "        registros = pd.concat([registros,instrument_positions])\n",
    "\n",
    "        # Concatena no df de custos\n",
    "        costs_df = pd.concat([costs_df,costs_position])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar data {data_col}: {e}\")\n",
    "    \n",
    "\n",
    "# Filtro pelas colunas de interesse e transforma quantidade, preço e aum em float\n",
    "registros = registros[columns_filter]\n",
    "registros.loc[:,['quantity','asset_value','exposure_value','price','dtd_ativo_fin']] = registros[['quantity','asset_value','exposure_value','price','dtd_ativo_fin']].astype(np.float64)\n",
    "\n",
    "# Transforma a coluna de custos em float e o id em int\n",
    "costs_df['financial_value']=costs_df['financial_value'].astype(float)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "----------- Filtro de contingencia enquanto não arruma as categorias no sistema ---------\n",
    "\n",
    "        soma as quantidades e notional pegando o primeiro book name para categorização\n",
    "\"\"\"\n",
    "registros = registros.groupby(['overview_date','portfolio_id','instrument_name']).agg({\n",
    "    'instrument_id':'first',\n",
    "    'book_name':'first',\n",
    "    'instrument_type':'first',\n",
    "    'quantity':'sum',\n",
    "    'price':'first',\n",
    "    'asset_value':'sum',\n",
    "    'exposure_value':'sum',\n",
    "    'dtd_ativo_fin': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Adiciono uma coluna com o NAV do fundo para o calculo de PNL\n",
    "registros = pd.merge(registros,funds_nav,left_on=['portfolio_id','overview_date'],right_on=['portfolio_id','date']).drop(columns='date')\n",
    "registros.rename(columns={\"net_asset_value\":\"portfolio_nav\"},inplace=True)\n",
    "\n",
    "# Filtro as colunas de custos\n",
    "costs_df = costs_df[['financial_value','attribution','book_name','category_name','origin_portfolio_id','origin_accounting_transaction_id','dtd_custos_fin','overview_date']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3dad9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = registros[['portfolio_id','instrument_name','book_name']].drop_duplicates()\n",
    "# duplicados = reg.groupby('instrument_name')['book_name'].nunique().reset_index()\n",
    "# duplicados = duplicados[duplicados['book_name'] > 1]['instrument_name']\n",
    "\n",
    "# # 2. Filtro final: ou está nos duplicados, ou é Padrão\n",
    "# df_filtrado = reg[\n",
    "#     (reg['instrument_name'].isin(duplicados)) |\n",
    "#     (reg['book_name'] == 'Padrão')\n",
    "# ]\n",
    "\n",
    "# df_filtrado = df_filtrado.merge(funds_name,on='portfolio_id',how='left')\n",
    "# df_filtrado[df_filtrado['name'].isin(all_funds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be55e218",
   "metadata": {},
   "source": [
    "Flatten o csv de posições em um novo csv 'explodido'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "31e894f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AlbertoCoppola\\AppData\\Local\\Temp\\ipykernel_17164\\2606223352.py:37: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  mult = notional/aum\n",
      "C:\\Users\\AlbertoCoppola\\AppData\\Local\\Temp\\ipykernel_17164\\2606223352.py:37: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mult = notional/aum\n"
     ]
    }
   ],
   "source": [
    "def explodir_portfolio(portfolio_id, data, todas_posicoes, todos_custos, visitados=None, notional = None, portfolio_origem_id=None, nivel = 0):\n",
    "    \"\"\"\n",
    "    - portfolio_id: o portfólio que estamos processando\n",
    "    - data: a data da posição\n",
    "    - todas_posicoes: DataFrame com todas as posições\n",
    "    - multiplicador: proporção da posição herdada\n",
    "    \"\"\"\n",
    "    \n",
    "    if visitados is None:\n",
    "        visitados = set()\n",
    "        # print(\"STARTING: \",portfolio_id)\n",
    "    if portfolio_id in visitados:\n",
    "        return [], pd.DataFrame()\n",
    "\n",
    "    visitados.add(portfolio_id)\n",
    "\n",
    "    # Filtra as posições desse portfolio na data\n",
    "    posicoes = todas_posicoes[\n",
    "        (todas_posicoes['overview_date'] == data) &\n",
    "        (todas_posicoes['portfolio_id'] == portfolio_id)\n",
    "    ]\n",
    "\n",
    "    # Filtra os custos do portfolio na data\n",
    "    custos = todos_custos[\n",
    "        (todos_custos['overview_date'] == data) &\n",
    "        (todos_custos['origin_portfolio_id'] == portfolio_id)\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Calculo do AUM total do fundo, soma ativos + custos\n",
    "    aum = posicoes.asset_value.sum() + custos.financial_value.sum()\n",
    "    # print(aum)\n",
    "    if notional is None:\n",
    "        notional = aum\n",
    "    # print(notional)\n",
    "    # Calculo um multiplicador para proporcionalizar as posições\n",
    "    mult = notional/aum\n",
    "\n",
    "    if portfolio_origem_id is None:\n",
    "        portfolio_origem_id = portfolio_id\n",
    "        # exposure = 1\n",
    "    else:\n",
    "        # print(\"Multiplicando por: \", round(exposure*100,4), \"%\")\n",
    "        posicoes.loc[:,['quantity','asset_value','exposure_value','dtd_ativo_fin']] = posicoes[['quantity','asset_value','exposure_value','dtd_ativo_fin']] * mult\n",
    "        custos.loc[:,['financial_value','dtd_custos_fin']] = custos.loc[:,['financial_value','dtd_custos_fin']] * mult\n",
    "\n",
    "    # Seta o portfolio_id para a origem e cria na tabela de custos\n",
    "    posicoes.loc[:,['portfolio_id']] = portfolio_origem_id\n",
    "    custos.loc[:,['root_portfolio']] = portfolio_origem_id\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for _, row in posicoes.iterrows():\n",
    "        row_portfolio_id = row['instrument_id']\n",
    "        if row.instrument_name in (all_funds):  # Checa se a linha é um fundo\n",
    "            # print(row.instrument_name)\n",
    "            # É um fundo investido, explodir recursivamente\n",
    "            sub_resultados, resultado_custo = explodir_portfolio(\n",
    "                row_portfolio_id,\n",
    "                data,\n",
    "                todas_posicoes,\n",
    "                todos_custos,\n",
    "                visitados=visitados,\n",
    "                notional = np.float64(row.asset_value),\n",
    "                portfolio_origem_id=portfolio_origem_id,\n",
    "                nivel = nivel + 1\n",
    "            )\n",
    "            resultados += sub_resultados\n",
    "            # Concatena no df de custos\n",
    "            custos = pd.concat([custos,resultado_custo])\n",
    "        else:\n",
    "            novo = row.copy()\n",
    "            novo['portfolio_origem'] = portfolio_id\n",
    "            novo[\"nivel\"] = nivel\n",
    "            resultados.append(novo)\n",
    "    \n",
    "    return resultados, custos\n",
    "\n",
    "todas_explodidas = pd.DataFrame()\n",
    "todos_custos_explodidos = pd.DataFrame()\n",
    "\n",
    "datas = registros.overview_date.unique()\n",
    "portfolios = registros['portfolio_id'].unique()\n",
    "\n",
    "for data in datas:\n",
    "    for portfolio in portfolios:\n",
    "        explodido, custo = explodir_portfolio(portfolio, data, registros, costs_df)\n",
    "        todas_explodidas = pd.concat([todas_explodidas,pd.DataFrame(explodido)])\n",
    "        todos_custos_explodidos = pd.concat([todos_custos_explodidos,custo])\n",
    "df_explodido = pd.DataFrame(todas_explodidas)\n",
    "\n",
    "# Subscrevo o portfolio_nav para referenciar ao portfolio_id original\n",
    "df_explodido = pd.merge(df_explodido.drop(columns=['portfolio_nav']),funds_nav,left_on=['portfolio_id','overview_date'],right_on=['portfolio_id','date'],how='left').drop(columns=['date'])\n",
    "\n",
    "todos_custos_explodidos = todos_custos_explodidos.groupby(['overview_date','root_portfolio','origin_portfolio_id','category_name']).agg({\n",
    "    'book_name':'first',\n",
    "    'financial_value':'sum',\n",
    "    'dtd_custos_fin':'sum'\n",
    "}).reset_index()\n",
    "\n",
    "df_explodido['pct_exposicao'] = df_explodido['exposure_value'].astype(float) / df_explodido['portfolio_nav'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1168a6",
   "metadata": {},
   "source": [
    "Tratamento de exceção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "00c8ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caixas Fundos viram CPR\n",
    "filtro = df_explodido['book_name'] == 'Caixa >> Caixa Fundos'\n",
    "df_explodido.loc[filtro, 'book_name'] = 'Caixas e Provisionamentos'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b28666",
   "metadata": {},
   "source": [
    "Quebra os grupos em colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "be31c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split da coluna pelo separador \" >> \"\n",
    "df_split = df_explodido['book_name'].str.split(' >> ', expand=True)\n",
    "\n",
    "# Renomeia as colunas de acordo com a quantidade de splits encontrados\n",
    "df_split.columns = [f'grupo_{i+1}' for i in range(df_split.shape[1])]\n",
    "\n",
    "# Junta com o df original (se quiser)\n",
    "pos_to_export = pd.concat([df_explodido, df_split], axis=1)\n",
    "\n",
    "pos_to_export.drop(columns='instrument_name',inplace=True)\n",
    "pos_to_export['overview_date'] = pd.to_datetime(pos_to_export['overview_date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b106aa8",
   "metadata": {},
   "source": [
    "Exportações"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de2fed",
   "metadata": {},
   "source": [
    "Concatena a tabela no csv de posições e custos, reprocessando as datas coincidentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df187a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    main_csv_position = pd.read_csv(\"portfolio_positions_exploded.csv\",parse_dates=['overview_date'])\n",
    "    main_csv_costs = pd.read_csv(\"portfolio_costs_exploded.csv\",parse_dates=['overview_date'])\n",
    "\n",
    "    main_csv_position.to_csv(\"portfolio_positions_exploded_backup.csv\", index=False)\n",
    "    main_csv_costs.to_csv(\"portfolio_costs_exploded_backup.csv\", index=False)\n",
    "\n",
    "    main_csv_portfoliopnl = pd.read_csv(\"portfolio_pnl_history.csv\",parse_dates=['date'])\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\" Para posições: \"\"\"\n",
    "    # Primeiro: remove do main_csv_position todas as linhas que têm datas que existem no pos_to_export\n",
    "    main_csv_filtrado = main_csv_position[~main_csv_position['overview_date'].isin(pos_to_export['overview_date'])]\n",
    "\n",
    "    # Segundo: concatena os dois\n",
    "    df_resultado = pd.concat([main_csv_filtrado, pos_to_export], ignore_index=True)\n",
    "\n",
    "    # Terceiro: opcional - reordenar por data, se quiser\n",
    "    df_resultado = df_resultado.sort_values('overview_date').reset_index(drop=True)\n",
    "\n",
    "    #Exporta CSV\n",
    "    df_resultado.to_csv(\"portfolio_positions_exploded.csv\", index=False)\n",
    "\n",
    "\n",
    "    \"\"\" Para custos: \"\"\"\n",
    "    # Primeiro: remove do main_csv_costs todas as linhas que têm datas que existem no todos_custos_explodidos\n",
    "    main_csv_filtrado = main_csv_costs[~main_csv_costs['overview_date'].isin(todos_custos_explodidos['overview_date'])]\n",
    "\n",
    "    # Segundo: concatena os dois\n",
    "    todos_custos_explodidos.loc[:,'book_name'] = 'Caixas e Provisionamentos'\n",
    "    df_resultado = pd.concat([main_csv_filtrado, todos_custos_explodidos], ignore_index=True)\n",
    "\n",
    "    # Terceiro: opcional - reordenar por data, se quiser\n",
    "    df_resultado = df_resultado.sort_values('overview_date').reset_index(drop=True)\n",
    "\n",
    "    #Exporta CSV\n",
    "    df_resultado.to_csv(\"portfolio_costs_exploded.csv\", index=False)\n",
    "\n",
    "    \"\"\" Para o pnl: \"\"\"\n",
    "    df_portfoliopnl = df_positions.loc[['date','portfolio_id','profitability_in_day','profitability_in_month','profitability_in_year']].T\n",
    "    df_portfoliopnl.date = pd.to_datetime(df_portfoliopnl.date)\n",
    "    \n",
    "    # Primeiro: remove do main_csv_portfoliopnl todas as linhas que têm datas que existem no df_portfoliopnl\n",
    "    main_csv_filtrado = main_csv_portfoliopnl[~main_csv_portfoliopnl['date'].isin(df_portfoliopnl['date'])]\n",
    "\n",
    "    # Segundo: concatena os dois\n",
    "    df_resultado = pd.concat([main_csv_filtrado, df_portfoliopnl], ignore_index=True)\n",
    "\n",
    "    # Terceiro: opcional - reordenar por data, se quiser\n",
    "    df_resultado = df_resultado.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    #Exporta CSV\n",
    "    df_resultado.to_csv(\"portfolio_pnl_history.csv\", index=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Not processed.\")\n",
    "    #Exporta CSV\n",
    "    # pos_to_export.to_csv(\"portfolio_positions_exploded.csv\", index=False)\n",
    "    # todos_custos_explodidos.loc[:,'book_name'] = 'CPR'\n",
    "    # todos_custos_explodidos.to_csv(\"portfolio_costs_exploded.csv\", index=False)\n",
    "\n",
    "    # #Exporta pnl diário de cada fundo fechado\n",
    "    # df_positions.loc[['date','portfolio_id','profitability_in_day','profitability_in_month','profitability_in_year']].T.to_csv(\"portfolio_pnl_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "63bfc68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para armazenar os pares (Investidor, Investida)\n",
    "# relacoes = []\n",
    "\n",
    "# # Iterar pelas colunas (fundos investidores)\n",
    "# for investidor in df_positions.columns:\n",
    "#     linha = pd.DataFrame(df_positions.loc['instrument_positions', investidor])  # obtém o dicionário da linha 'instrument_positions' e transforma em Dataframe\n",
    "#     # linha = linha[linha.instrument_type==3] # filtra para só buscar posições em portfolios\n",
    "#     for investida in linha.instrument_name:\n",
    "#         relacoes.append((df_positions[investidor].loc['name'],investida))\n",
    "    \n",
    "\n",
    "# # Criar DataFrame final\n",
    "# df_relacoes = pd.DataFrame(relacoes, columns=['Cliente', 'Carteira'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cd6367e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "funds_nav.to_csv(\"funds_nav.csv\", index=False)\n",
    "\n",
    "# df_relacoes.to_csv('funds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6bb042d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_existente = pd.read_csv(\"instruments.csv\")\n",
    "except FileNotFoundError:\n",
    "    df_explodido[['instrument_id','instrument_name']].drop_duplicates().to_csv(\"instruments.csv\", index=False)\n",
    "\n",
    "# Gera df com os instrumentos do novo arquivo\n",
    "df_novos = df_explodido[['instrument_id', 'instrument_name']].drop_duplicates()\n",
    "\n",
    "# Junta os dois, mas só adiciona os que NÃO estão no existente\n",
    "df_final = pd.concat([df_existente, df_novos]).drop_duplicates(subset=['instrument_id'])\n",
    "\n",
    "# Salva o resultado atualizado\n",
    "df_final.to_csv(\"instruments.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
